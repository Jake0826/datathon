{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzjeYEwjMxc1"
      },
      "source": [
        "# **4.2 - Image Classification of Dogs vs. Cats vs. Horses**\n",
        "**15 points**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcRoTsByDa9N"
      },
      "source": [
        "### **Goals**\n",
        "\n",
        "4.2 - Know how to use weights and biases with neural networks\n",
        "\n",
        "4.4 - Understand backwards propogation\n",
        "\n",
        "4.5 - Understand how to prevent overfitting\n",
        "\n",
        "4.6 - Know how to interpret loss data/graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NxhfD7D14b"
      },
      "source": [
        "### **Step 1 -** **Import libraries**\n",
        "Completed for you"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOI2SZ7BDTzl",
        "outputId": "690f99b8-6f4b-40b9-bcd5-11fa2b550921"
      },
      "source": [
        "# put your step 1 code here - done for you\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "print(tf.test.gpu_device_name())\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLPTag5vD6Lo"
      },
      "source": [
        "### **Step 2 - Download two sets of images (dogs and cats, horses)**\n",
        "\n",
        "**Extension idea: Add humans or other animals or use different ones (Tigers vs Lions vs Cheetahs?)\n",
        "\n",
        "Completed for you"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZT2HYnJus28",
        "outputId": "18bea8fe-99e7-4671-a3bc-aa6a2f0a7e2d"
      },
      "source": [
        "# put your step 2 code here - completed for you\n",
        "\n",
        "# Cats and Dogs\n",
        "!wget -nc --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
        "    -O \"cats-and-dogs.zip\"\n",
        "\n",
        "# Horses\n",
        "!gdown --id  11NB8Yu0D158WSHms_JHYzsA_g7EOWjRm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-12 07:11:17--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 184.84.168.111, 2600:1407:b800:18a::e59, 2600:1407:b800:19c::e59\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|184.84.168.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824894548 (787M) [application/octet-stream]\n",
            "Saving to: ‘cats-and-dogs.zip’\n",
            "\n",
            "cats-and-dogs.zip   100%[===================>] 786.68M   143MB/s    in 5.5s    \n",
            "\n",
            "2021-11-12 07:11:23 (143 MB/s) - ‘cats-and-dogs.zip’ saved [824894548/824894548]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11NB8Yu0D158WSHms_JHYzsA_g7EOWjRm\n",
            "To: /content/horsedataset.zip\n",
            "100% 77.5M/77.5M [00:00<00:00, 166MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6rB_dpnFx80"
      },
      "source": [
        "### **Step 3 - Extract files and move into sub folders**\n",
        "2 points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RryEaPNFGJWP"
      },
      "source": [
        "# put your step 3 code here - completed for you\n",
        "\n",
        "# Cats and Dogs\n",
        "local_zip = 'cats-and-dogs.zip'\n",
        "zip_ref  = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('')\n",
        "zip_ref.close()\n",
        "\n",
        "# Horses\n",
        "local_zip = 'horsedataset.zip'\n",
        "zip_ref  = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('horse')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYHMGmi24Mk3"
      },
      "source": [
        "!mv /content/PetImages/ /content/Images/\n",
        "!mv /content/horse/ /content/Images/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0lxLgPbO4mW"
      },
      "source": [
        "# **Step 4 - Balance categories**\n",
        "\n",
        "**Optional extension**\n",
        "\n",
        "If the categories are out of balance, the model won't work correctly.\n",
        "\n",
        "We can trim them to balance the classes by ensuring the same number of images in each folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyy28Q-YNPpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b6ad99-83bf-482b-9540-09d2c67962e3"
      },
      "source": [
        "# put your step 4 code here\n",
        "# balance up the directories\n",
        "!rm /content/Images/Dog/[1-9]???.jpg \n",
        "!rm /content/Images/Dog/1????.jpg | wc -l\n",
        "!rm /content/Images/Dog/[0-5]??.jpg\n",
        "!rm /content/Images/Cat/[1-9]???.jpg \n",
        "!rm /content/Images/Cat/1????.jpg | wc -l\n",
        "!rm /content/Images/horse/07????.png| wc -l\n",
        "!rm /content/Images/horse/06_09?.png| wc -l\n",
        "# try running model with and without balancing\n",
        "# count the number of files in each directory\n",
        "!ls -l /content/Images/Cat/ | wc -l\n",
        "!ls -l /content/Images/Dog/ | wc -l\n",
        "!ls -l /content/Images/horse/ | wc -l\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1002\n",
            "502\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl_gOzVUqWTy"
      },
      "source": [
        "## **Step 5 - Check for corrupt files and remove if necessary**\n",
        "3 points\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apbx9frn2AzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc29df7-fe8e-4318-b1c8-3b1afddbbdec"
      },
      "source": [
        "# put your step 5 code here\n",
        "import os\n",
        "import cv2\n",
        "bad_list=[]\n",
        "dir=r'/content/Images/'\n",
        "class_list=os.listdir(dir) # create a list of the sub directories in the directory ie train or test\n",
        "print (class_list)\n",
        "for klass in class_list: # iterate through the two classes\n",
        "            class_path=os.path.join(dir, klass) # path to class directory\n",
        "            #print(class_path)\n",
        "            file_list=os.listdir(class_path) # create list of files in class directory\n",
        "            for f in file_list: # iterate through the files\n",
        "                fpath=os.path.join (class_path,f)\n",
        "                index=f.rfind('.') # find index of period infilename\n",
        "                ext=f[index+1:] # get the files extension\n",
        "                if ext  not in ['jpg', 'png', 'bmp', 'gif']:\n",
        "                    print(f'file {fpath}  has an invalid extension {ext}')\n",
        "                    bad_list.append(fpath)                    \n",
        "                else:\n",
        "                    try:\n",
        "                        img=cv2.imread(fpath)\n",
        "                        size=img.shape\n",
        "                    except:\n",
        "                        print(f'file {fpath} is not a valid image file ')\n",
        "                        bad_list.append(fpath)\n",
        "                       \n",
        "print (bad_list)\n",
        "for image in bad_list:\n",
        "  os.remove(image)               \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['horse', 'Cat', 'Dog']\n",
            "file /content/Images/Cat/850.jpg is not a valid image file \n",
            "file /content/Images/Cat/936.jpg is not a valid image file \n",
            "file /content/Images/Cat/140.jpg is not a valid image file \n",
            "file /content/Images/Cat/666.jpg is not a valid image file \n",
            "file /content/Images/Cat/660.jpg is not a valid image file \n",
            "file /content/Images/Cat/Thumbs.db  has an invalid extension db\n",
            "file /content/Images/Dog/Thumbs.db  has an invalid extension db\n",
            "['/content/Images/Cat/850.jpg', '/content/Images/Cat/936.jpg', '/content/Images/Cat/140.jpg', '/content/Images/Cat/666.jpg', '/content/Images/Cat/660.jpg', '/content/Images/Cat/Thumbs.db', '/content/Images/Dog/Thumbs.db']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tChXi5DVrBiV"
      },
      "source": [
        "## **Step 6 - Create the training and validation set from the image directories**\n",
        "\n",
        "\n",
        "We now have three categories. **What do we need to change here?**\n",
        "\n",
        "3 points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDPp3YWwwxBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa5f4f6-eb14-447b-be52-5a2f1d19efb0"
      },
      "source": [
        "# put your step 6 code here\n",
        "img_height=224\n",
        "img_width=224\n",
        "batch_size=250\n",
        "train_data_dir = \"/content/Images/\"\n",
        "train_datagen = ImageDataGenerator(rescale=1./255.,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2) # set validation split\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir, # same directory as training data\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1596 images belonging to 3 classes.\n",
            "Found 398 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDZIF1P9rqR9"
      },
      "source": [
        "## **Step 7 - Load the VGG16 model and print model summary**\n",
        "2 points\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao3_rSItwxKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7768461-e330-40ed-9376-f37e8e57bcdd"
      },
      "source": [
        "# put your step 7 code here\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "# load model\n",
        "model = VGG16() \n",
        "# summarize the model\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 7s 0us/step\n",
            "553476096/553467096 [==============================] - 7s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k16mUuM3r3y9"
      },
      "source": [
        "## **Step 8 - Define and compile our version of the model and print model summary**\n",
        "\n",
        "**What do we need to change here compared to Cats vs Dogs?**\n",
        "\n",
        "2 points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fZSvrM8xsuw"
      },
      "source": [
        "# define cnn model\n",
        "def define_model():\n",
        "\t# load model\n",
        "\tmodel = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "\t# mark loaded layers as not trainable\n",
        "\tfor layer in model.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\t# add new classifier layers\n",
        "\tflat1 = Flatten()(model.layers[-1].output)\n",
        "\tclass1 = Dense(256, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        " \n",
        "  # output\n",
        "\toutput = Dense(3, activation='softmax')(class1)\n",
        " \n",
        "\t# define new model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.001, momentum=0.9)\n",
        " \n",
        "  # compile model\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\tmodel.summary()\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INrTx7ShsYVK"
      },
      "source": [
        "## **Step 9 - Plot learning curves so we can analyze loss and accuracy**\n",
        "\n",
        "This is useful as we can see how the model performs as the number of epochs increase.\n",
        "\n",
        "1 point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aefFjRq4sq3x"
      },
      "source": [
        "# put your step 9 code here\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "  # plot loss\n",
        "  pyplot.subplot(211)\n",
        "  pyplot.title('Cross Entropy Loss')\n",
        "  pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "  #pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "  # plot accuracy\n",
        "  pyplot.subplot(212)\n",
        "  pyplot.title('Classification Accuracy')\n",
        "  pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        " # pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "  # save plot to file\n",
        "  filename = sys.argv[0].split('/')[-1]\n",
        "  pyplot.savefig(filename + '_plot.png')\n",
        "  pyplot.show()\n",
        "  pyplot.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN95aP1Us5kA"
      },
      "source": [
        "## **Step 10 - Train our version of the model**\n",
        "\n",
        "Completed for you"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQusWDjux_tI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81707163-8042-4944-84f5-98a809317f00"
      },
      "source": [
        "# put your step 10 code here\n",
        "# done for you\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model = define_model()\n",
        "\n",
        "nb_epochs=5\n",
        "history=model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = nb_epochs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               6422784   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,138,243\n",
            "Trainable params: 6,423,555\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22kZq5LstO2Q"
      },
      "source": [
        "## **Step 11 - Show the accuracy of our version of the model**\n",
        "\n",
        "1 point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhwvRSZR2wVx"
      },
      "source": [
        "# put your step 11 code here\n",
        "\n",
        "_, acc = model.evaluate(validation_generator, steps=len(validation_generator), verbose=0)\n",
        "print('> %.3f' % (acc * 100.0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEVJPLn_tcH9"
      },
      "source": [
        "## **Step 12 - Summarize the performance of our model**\n",
        "\n",
        "1 point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxSIJw4IPAMN"
      },
      "source": [
        "# put your step 12 code here\n",
        "summarize_diagnostics(history)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th84IJj2ItTc"
      },
      "source": [
        "# **Step 13 - Check if your model categorizes correctly**\n",
        "\n",
        "Completed for you"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTpQd1esI0h4"
      },
      "source": [
        "# Here's a codeblock just for fun. You should be able to upload an image here \n",
        "# and have it classified without crashing\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(224, 224))\n",
        "  pyplot.imshow(img)\n",
        "  pyplot.show()\n",
        "\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  print(validation_generator.class_indices)\n",
        "  i = 0\n",
        "\n",
        "  for label in validation_generator.class_indices:\n",
        "        print(\"\\t%s ==> %f\" % (label, classes[0][i]))\n",
        "        i = i + 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}